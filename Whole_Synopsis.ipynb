{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>walk-me-out-in-the-morning-dew</td>\n",
       "      <td>walk-me-out-in-the-morning-dew</td>\n",
       "      <td>Walk me out in the mornin' dew my honey\\nWalk ...</td>\n",
       "      <td>Walk mornin ' dew honey Walk mornin ' dew toda...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>helping-you-hate-me</td>\n",
       "      <td>helping-you-hate-me</td>\n",
       "      <td>I know it's a clich\\nWhen they say we were doo...</td>\n",
       "      <td>I know 's clich When say doomed could start We...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>trailerpark</td>\n",
       "      <td>trailerpark</td>\n",
       "      <td>Fist out\\nFist out for the five\\nFist out\\nI'm...</td>\n",
       "      <td>Fist Fist five Fist I 'm country boy I got sou...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>when-i-see-his-face</td>\n",
       "      <td>when-i-see-his-face</td>\n",
       "      <td>It could be any day now\\nIt could be any day\\n...</td>\n",
       "      <td>It could day It could day It could day Nothing...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>call-your-name</td>\n",
       "      <td>call-your-name</td>\n",
       "      <td>I call your name but you're not there\\nWas I t...</td>\n",
       "      <td>I call name 're Was I blame unfair Oh , I ca n...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            song                          artist  \\\n",
       "0           1  walk-me-out-in-the-morning-dew  walk-me-out-in-the-morning-dew   \n",
       "1           2             helping-you-hate-me             helping-you-hate-me   \n",
       "2           3                     trailerpark                     trailerpark   \n",
       "3           4             when-i-see-his-face             when-i-see-his-face   \n",
       "4           6                  call-your-name                  call-your-name   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Walk me out in the mornin' dew my honey\\nWalk ...   \n",
       "1  I know it's a clich\\nWhen they say we were doo...   \n",
       "2  Fist out\\nFist out for the five\\nFist out\\nI'm...   \n",
       "3  It could be any day now\\nIt could be any day\\n...   \n",
       "4  I call your name but you're not there\\nWas I t...   \n",
       "\n",
       "                                      cleaned_lyrics genre  \n",
       "0  Walk mornin ' dew honey Walk mornin ' dew toda...  Rock  \n",
       "1  I know 's clich When say doomed could start We...  Rock  \n",
       "2  Fist Fist five Fist I 'm country boy I got sou...  Rock  \n",
       "3  It could day It could day It could day Nothing...  Rock  \n",
       "4  I call name 're Was I blame unfair Oh , I ca n...  Rock  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df = pd.read_excel('Data_TL.xlsx')\n",
    "print(len(df.genre.unique()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rock', 'Pop', 'Metal', 'HipHop', 'Country', 'Jazz', 'Electronic',\n",
       "       'RythBlues', 'Indie', 'Folk'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cleaned_lyrics = df.cleaned_lyrics.astype(str)\n",
    "df.genre = df.genre.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518460, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Rock = df[df.genre == 'Rock'][:1000]\n",
    "df_Pop = df[df.genre == 'Pop'][:1000]\n",
    "df_Metal = df[df.genre == 'Metal'][:1000]\n",
    "df_HipHop = df[df.genre == 'HipHop'][:1000]\n",
    "df_Country = df[df.genre == 'Country'][:1000]\n",
    "df_Jazz = df[df.genre == 'Jazz'][:1000]\n",
    "df_Electronic = df[df.genre == 'Electronic'][:1000]\n",
    "df_RythBlues = df[df.genre == 'RythBlues'][:1000]\n",
    "df_Indie = df[df.genre == 'Indie'][:1000]\n",
    "df_Folk = df[df.genre == 'Folk'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "df_new = pd.concat([df_Rock,df_Pop,df_Metal,df_HipHop,df_Country,df_Jazz,df_Electronic,df_RythBlues,df_Indie,df_Folk],axis = 0)\n",
    "df_new.cleaned_lyrics = df_new.cleaned_lyrics.apply(lambda x:' '.join([word for word in x.split() if len(word) > 3]))\n",
    "df_new.cleaned_lyrics = df_new.cleaned_lyrics.apply(lambda x:' '.join([word for word in x.split() if word not in punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_excel('10000data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Walk mornin honey Walk mornin today walk mornin honey walk mornin today thought heard baby mornin' thought heard baby today hear baby mornin' hear baby today Where people gone honey Where people gone today Well need worryin people never people anyway thought heard young moan mornin' thought heard young moan today thought heard young moan mornin' walk morning today\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.cleaned_lyrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['walk', 'mornin', 'honey', 'walk', 'mornin', 'today', 'walk', 'mornin', 'honey', 'walk', 'mornin', 'today', 'thought', 'heard', 'baby', 'mornin', 'thought', 'heard', 'baby', 'today', 'hear', 'baby', 'mornin', 'hear', 'baby', 'today', 'where', 'people', 'gone', 'honey', 'where', 'people', 'gone', 'today', 'well', 'need', 'worryin', 'people', 'never', 'people', 'anyway', 'thought', 'heard', 'young', 'moan', 'mornin', 'thought', 'heard', 'young', 'moan', 'today', 'thought', 'heard', 'young', 'moan', 'mornin', 'walk', 'morning', 'today']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = df_new.cleaned_lyrics.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bigram and Trigram Models\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "# or do\n",
    "# !conda install -c conda-forge spacy-model-en_core_web_md \n",
    "# and use nlp=spacy.load('en_core_web_sm') instead in below function.\n",
    "def process_words(texts, stop_words=stop_words):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]   \n",
    "    return texts\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.057*\"know\" + 0.043*\"baby\" + 0.036*\"want\" + 0.036*\"yeah\" + 0.030*\"girl\" + '\n",
      "  '0.022*\"love\" + 0.022*\"make\" + 0.019*\"good\" + 0.019*\"cause\" + 0.019*\"like\" + '\n",
      "  '0.016*\"need\" + 0.015*\"come\" + 0.014*\"feel\" + 0.013*\"right\" + 0.012*\"tell\" + '\n",
      "  '0.011*\"time\" + 0.010*\"well\" + 0.010*\"take\" + 0.009*\"give\" + 0.008*\"talk\" + '\n",
      "  '0.008*\"alright\" + 0.007*\"please\" + 0.007*\"stop\" + 0.007*\"keep\" + '\n",
      "  '0.007*\"call\"'),\n",
      " (1,\n",
      "  '0.023*\"time\" + 0.019*\"never\" + 0.017*\"know\" + 0.016*\"could\" + 0.014*\"back\" '\n",
      "  '+ 0.014*\"take\" + 0.013*\"would\" + 0.012*\"thing\" + 0.011*\"life\" + '\n",
      "  '0.010*\"think\" + 0.010*\"said\" + 0.009*\"still\" + 0.009*\"like\" + 0.008*\"find\" '\n",
      "  '+ 0.008*\"cause\" + 0.008*\"nothing\" + 0.007*\"always\" + 0.007*\"home\" + '\n",
      "  '0.007*\"well\" + 0.007*\"friend\" + 0.007*\"mind\" + 0.007*\"something\" + '\n",
      "  '0.006*\"believe\" + 0.006*\"wrong\" + 0.006*\"tell\"'),\n",
      " (2,\n",
      "  '0.059*\"love\" + 0.025*\"come\" + 0.019*\"away\" + 0.019*\"heart\" + 0.016*\"long\" + '\n",
      "  '0.016*\"night\" + 0.010*\"alone\" + 0.010*\"gone\" + 0.010*\"home\" + 0.010*\"world\" '\n",
      "  '+ 0.010*\"know\" + 0.010*\"around\" + 0.009*\"song\" + 0.009*\"never\" + '\n",
      "  '0.009*\"dream\" + 0.007*\"time\" + 0.007*\"waiting\" + 0.007*\"true\" + '\n",
      "  '0.007*\"like\" + 0.007*\"hold\" + 0.007*\"lonely\" + 0.006*\"hear\" + 0.006*\"sweet\" '\n",
      "  '+ 0.006*\"could\" + 0.006*\"sing\"'),\n",
      " (3,\n",
      "  '0.010*\"child\" + 0.008*\"hand\" + 0.008*\"soul\" + 0.007*\"black\" + 0.007*\"upon\" '\n",
      "  '+ 0.006*\"must\" + 0.006*\"lord\" + 0.006*\"life\" + 0.006*\"land\" + 0.006*\"fire\" '\n",
      "  '+ 0.006*\"death\" + 0.005*\"dead\" + 0.005*\"light\" + 0.005*\"earth\" + '\n",
      "  '0.005*\"burn\" + 0.004*\"pain\" + 0.004*\"world\" + 0.004*\"stand\" + 0.004*\"blood\" '\n",
      "  '+ 0.004*\"lead\" + 0.004*\"mother\" + 0.004*\"born\" + 0.004*\"hear\" + '\n",
      "  '0.004*\"body\" + 0.004*\"fear\"'),\n",
      " (4,\n",
      "  '0.026*\"like\" + 0.008*\"well\" + 0.008*\"head\" + 0.006*\"back\" + 0.006*\"town\" + '\n",
      "  '0.006*\"come\" + 0.006*\"ride\" + 0.006*\"street\" + 0.005*\"came\" + 0.005*\"money\" '\n",
      "  '+ 0.005*\"play\" + 0.005*\"little\" + 0.004*\"night\" + 0.004*\"outta\" + '\n",
      "  '0.004*\"till\" + 0.004*\"rock\" + 0.004*\"cause\" + 0.004*\"kick\" + 0.004*\"went\" + '\n",
      "  '0.004*\"young\" + 0.004*\"high\" + 0.004*\"work\" + 0.004*\"name\" + 0.004*\"good\" + '\n",
      "  '0.004*\"drink\"'),\n",
      " (5,\n",
      "  '0.030*\"dance\" + 0.028*\"tonight\" + 0.026*\"light\" + 0.026*\"little\" + '\n",
      "  '0.025*\"wild\" + 0.024*\"christmas\" + 0.021*\"everybody\" + 0.019*\"shine\" + '\n",
      "  '0.015*\"sleep\" + 0.015*\"move\" + 0.014*\"free\" + 0.014*\"music\" + 0.013*\"turn\" '\n",
      "  '+ 0.013*\"slow\" + 0.009*\"engine\" + 0.007*\"mystery\" + 0.007*\"dilly_dilly\" + '\n",
      "  '0.006*\"dancing\" + 0.005*\"spirit\" + 0.005*\"payment\" + 0.005*\"bright\" + '\n",
      "  '0.005*\"fashioned_waltz\" + 0.005*\"loud\" + 0.004*\"leavin_first_caboose\" + '\n",
      "  '0.004*\"basement\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=10,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics(num_words = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.125550533999842\n",
      "\n",
      "Coherence Score:  0.3657304133103542\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topic(topicid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>time, never, know, could, back, take, would, t...</td>\n",
       "      <td>[walk, mornin, honey, walk, mornin, today, wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>time, never, know, could, back, take, would, t...</td>\n",
       "      <td>[know, clich, doomed, could, start, making, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6336</td>\n",
       "      <td>child, hand, soul, black, upon, must, lord, li...</td>\n",
       "      <td>[fist, fist, five, fist, country, soul, sleep,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>time, never, know, could, back, take, would, t...</td>\n",
       "      <td>[could, could, could, nothing, going, face, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3848</td>\n",
       "      <td>know, baby, want, yeah, girl, love, make, good...</td>\n",
       "      <td>[call, name, blame, unfair, sleep, night, sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6675</td>\n",
       "      <td>like, well, head, back, town, come, ride, stre...</td>\n",
       "      <td>[love, already, wooed, chorus, cause, like, ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>child, hand, soul, black, upon, must, lord, li...</td>\n",
       "      <td>[resident, mockery, give, hour, magic, purple,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>like, well, head, back, town, come, ride, stre...</td>\n",
       "      <td>[midst, offered, peace, came, like, lover, eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>time, never, know, could, back, take, would, t...</td>\n",
       "      <td>[gone, advice, side, road, dying, breath, yell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>time, never, know, could, back, take, would, t...</td>\n",
       "      <td>[wrote_letter, nothing, asked, somebody, could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             1.0              0.3542   \n",
       "1            1             1.0              0.5790   \n",
       "2            2             3.0              0.6336   \n",
       "3            3             1.0              0.5197   \n",
       "4            4             0.0              0.3848   \n",
       "5            5             4.0              0.6675   \n",
       "6            6             3.0              0.3470   \n",
       "7            7             4.0              0.5655   \n",
       "8            8             1.0              0.5226   \n",
       "9            9             1.0              0.7136   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  time, never, know, could, back, take, would, t...   \n",
       "1  time, never, know, could, back, take, would, t...   \n",
       "2  child, hand, soul, black, upon, must, lord, li...   \n",
       "3  time, never, know, could, back, take, would, t...   \n",
       "4  know, baby, want, yeah, girl, love, make, good...   \n",
       "5  like, well, head, back, town, come, ride, stre...   \n",
       "6  child, hand, soul, black, upon, must, lord, li...   \n",
       "7  like, well, head, back, town, come, ride, stre...   \n",
       "8  time, never, know, could, back, take, would, t...   \n",
       "9  time, never, know, could, back, take, would, t...   \n",
       "\n",
       "                                                Text  \n",
       "0  [walk, mornin, honey, walk, mornin, today, wal...  \n",
       "1  [know, clich, doomed, could, start, making, he...  \n",
       "2  [fist, fist, five, fist, country, soul, sleep,...  \n",
       "3  [could, could, could, nothing, going, face, te...  \n",
       "4  [call, name, blame, unfair, sleep, night, sinc...  \n",
       "5  [love, already, wooed, chorus, cause, like, ro...  \n",
       "6  [resident, mockery, give, hour, magic, purple,...  \n",
       "7  [midst, offered, peace, came, like, lover, eas...  \n",
       "8  [gone, advice, side, road, dying, breath, yell...  \n",
       "9  [wrote_letter, nothing, asked, somebody, could...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num,topn = 25)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_excel('Document-Topic-Music.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>know, baby, want, yeah, girl, love, make, good, cause, like, need, come, feel, right, tell, time...</td>\n",
       "      <td>[told, told, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>time, never, know, could, back, take, would, thing, life, think, said, still, like, find, cause,...</td>\n",
       "      <td>[something, something, something, something, something, something, something, something, somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>love, come, away, heart, long, night, alone, gone, home, world, know, around, song, never, dream...</td>\n",
       "      <td>[around, world, around, world, around, world, around, world, around, world, around, world, aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9624</td>\n",
       "      <td>child, hand, soul, black, upon, must, lord, life, land, fire, death, dead, light, earth, burn, p...</td>\n",
       "      <td>[suffocate, light, throw, stone, kill, truth, deception, must, absolute, rape, mind, sharpest, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9466</td>\n",
       "      <td>like, well, head, back, town, come, ride, street, came, money, play, little, night, outta, till,...</td>\n",
       "      <td>[bang, like, bang, shake_shake, freak, love, freak_work_twerk, bang, like, bang, shake_shake, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>dance, tonight, light, little, wild, christmas, everybody, shine, sleep, move, free, music, turn...</td>\n",
       "      <td>[human_arntcha_comin, human_arntcha_comin, human_arntcha_comin, human_arntcha_comin, human_arntc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9872   \n",
       "1        1.0              0.9886   \n",
       "2        2.0              0.9971   \n",
       "3        3.0              0.9624   \n",
       "4        4.0              0.9466   \n",
       "5        5.0              0.9490   \n",
       "\n",
       "                                                                                              Keywords  \\\n",
       "0  know, baby, want, yeah, girl, love, make, good, cause, like, need, come, feel, right, tell, time...   \n",
       "1  time, never, know, could, back, take, would, thing, life, think, said, still, like, find, cause,...   \n",
       "2  love, come, away, heart, long, night, alone, gone, home, world, know, around, song, never, dream...   \n",
       "3  child, hand, soul, black, upon, must, lord, life, land, fire, death, dead, light, earth, burn, p...   \n",
       "4  like, well, head, back, town, come, ride, street, came, money, play, little, night, outta, till,...   \n",
       "5  dance, tonight, light, little, wild, christmas, everybody, shine, sleep, move, free, music, turn...   \n",
       "\n",
       "                                                                                   Representative Text  \n",
       "0  [told, told, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay, stay,...  \n",
       "1  [something, something, something, something, something, something, something, something, somethi...  \n",
       "2  [around, world, around, world, around, world, around, world, around, world, around, world, aroun...  \n",
       "3  [suffocate, light, throw, stone, kill, truth, deception, must, absolute, rape, mind, sharpest, t...  \n",
       "4  [bang, like, bang, shake_shake, freak, love, freak_work_twerk, bang, like, bang, shake_shake, fr...  \n",
       "5  [human_arntcha_comin, human_arntcha_comin, human_arntcha_comin, human_arntcha_comin, human_arntc...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.to_excel('Topic-Contribution-Music.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3b6d01738cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/Users/adwait/Desktop/Mallet/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12,\n",
      "  [('baby', 0.1989782199515999),\n",
      "   ('yeah', 0.17119297302142153),\n",
      "   ('give', 0.10172985569597562),\n",
      "   ('girl', 0.03492575662513818),\n",
      "   ('woman', 0.03292402378178124),\n",
      "   ('touch', 0.03071315467120791),\n",
      "   ('care', 0.021690418571300527),\n",
      "   ('mama', 0.01696991425413044),\n",
      "   ('good', 0.0169400376445281),\n",
      "   ('head', 0.016671148158107017)]),\n",
      " (9,\n",
      "  [('back', 0.1774438834478983),\n",
      "   ('show', 0.04194294056816108),\n",
      "   ('move', 0.03731978830829126),\n",
      "   ('play', 0.03303120627775412),\n",
      "   ('watch', 0.0298071658860028),\n",
      "   ('game', 0.026066062412555508),\n",
      "   ('hard', 0.02402822556116552),\n",
      "   ('slow', 0.020043798284567187),\n",
      "   ('alive', 0.019952551858385546),\n",
      "   ('lose', 0.018675101891842568)]),\n",
      " (2,\n",
      "  [('nigga', 0.044144911559853686),\n",
      "   ('shit', 0.03455763224265504),\n",
      "   ('fuck', 0.027141687962452605),\n",
      "   ('bitch', 0.017303869987139016),\n",
      "   ('money', 0.015767232884034005),\n",
      "   ('yall', 0.01317833341684622),\n",
      "   ('real', 0.007983831905262983),\n",
      "   ('street', 0.00638038449332732),\n",
      "   ('ima', 0.006263466452873678),\n",
      "   ('niggaz', 0.006063035526381721)]),\n",
      " (4,\n",
      "  [('friend', 0.03641894581224768),\n",
      "   ('black', 0.03215443225109456),\n",
      "   ('dead', 0.02791834878034912),\n",
      "   ('hate', 0.024336157388980498),\n",
      "   ('sleep', 0.022971513049411498),\n",
      "   ('head', 0.01998635355660431),\n",
      "   ('line', 0.018991300392335247),\n",
      "   ('kill', 0.013873884118951497),\n",
      "   ('skin', 0.011400466253482686),\n",
      "   ('hole', 0.008244726218229375)]),\n",
      " (7,\n",
      "  [('dream', 0.07238879841619568),\n",
      "   ('blue', 0.04365041351342721),\n",
      "   ('sweet', 0.04135134272120573),\n",
      "   ('heaven', 0.02845100105374078),\n",
      "   ('call', 0.025928409490053327),\n",
      "   ('moon', 0.017722004023373886),\n",
      "   ('angel', 0.01724303094166108),\n",
      "   ('beautiful', 0.016540537088482295),\n",
      "   ('dear', 0.016380879394578024),\n",
      "   ('follow', 0.015167480920905578)]),\n",
      " (19,\n",
      "  [('hand', 0.07477016175957174),\n",
      "   ('year', 0.025253112998952637),\n",
      "   ('wind', 0.023565693005935064),\n",
      "   ('cold', 0.021994646805539392),\n",
      "   ('happy', 0.02144187129058536),\n",
      "   ('rain', 0.018561619923193298),\n",
      "   ('land', 0.01841615268241592),\n",
      "   ('water', 0.01765972303037356),\n",
      "   ('mother', 0.01652507855231002),\n",
      "   ('tree', 0.015041312696380775)]),\n",
      " (18,\n",
      "  [('world', 0.11492031304458034),\n",
      "   ('high', 0.0443585437723773),\n",
      "   ('side', 0.03596210512974874),\n",
      "   ('till', 0.034726402838946804),\n",
      "   ('ride', 0.03342733119989861),\n",
      "   ('fight', 0.028864738126168372),\n",
      "   ('foot', 0.022401064605050536),\n",
      "   ('ground', 0.0206900922024017),\n",
      "   ('wild', 0.020341560787047304),\n",
      "   ('white', 0.00982224897816926)]),\n",
      " (5,\n",
      "  [('life', 0.14814708002883922),\n",
      "   ('live', 0.05162220620043259),\n",
      "   ('lost', 0.041528478731074264),\n",
      "   ('left', 0.03330930064888248),\n",
      "   ('remember', 0.031146359048305695),\n",
      "   ('hope', 0.02633020908435472),\n",
      "   ('told', 0.024023071377072817),\n",
      "   ('chance', 0.022754145638067772),\n",
      "   ('truth', 0.021369863013698632),\n",
      "   ('living', 0.01961067051189618)]),\n",
      " (1,\n",
      "  [('hold', 0.08013948429202124),\n",
      "   ('stay', 0.06136029176530808),\n",
      "   ('leave', 0.05086697805361828),\n",
      "   ('stand', 0.03791029496448909),\n",
      "   ('close', 0.032919572589417107),\n",
      "   ('forever', 0.030296244161494658),\n",
      "   ('soul', 0.02696909591144667),\n",
      "   ('child', 0.022330283447437455),\n",
      "   ('strong', 0.02121057009405592),\n",
      "   ('shine', 0.020602725702220232)]),\n",
      " (10,\n",
      "  [('chorus', 0.06808056069409343),\n",
      "   ('stop', 0.049607878898413275),\n",
      "   ('call', 0.029467705375055366),\n",
      "   ('crazy', 0.018238190771475468),\n",
      "   ('bout', 0.016075662437143378),\n",
      "   ('goin', 0.015893280529428625),\n",
      "   ('verse', 0.013939188661056251),\n",
      "   ('nothin', 0.012975170005992548),\n",
      "   ('comin', 0.011516114744274511),\n",
      "   ('repeat', 0.011437951069539615)])]\n",
      "\n",
      "Coherence Score:  0.34978778634359176\n"
     ]
    }
   ],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_ready, start=2, limit=20, step=6)\n",
    "# Show graph\n",
    "limit=20; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
